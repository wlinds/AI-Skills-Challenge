{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In 1950, the British mathematician Alan Turing devised the Imitation Game, which has become known as the Turing Test and hypothesizes that if a dialog is natural enough, you might not know whether you're conversing with a human or a computer. As artificial intelligence (AI) grows ever more sophisticated, this kind of conversational interaction with applications and digital assistants is becoming more and more common, and in specific scenarios can result in human-like interactions with AI agents. Common scenarios for this kind of solution include customer support applications, reservation systems, and home automation among others.\n",
    "\n",
    "To realize the aspiration of the imitation game, computers need not only to be able to accept language as input (either in text or audio format), but also to be able to interpret the semantic meaning of the input - in other words, understand what is being said.\n",
    "Microsoft Azure supports conversational language understanding through Azure AI Language service. One example using conversational language understanding is an application that's able to turn devices on and off based on speech. Many types of tasks involving command and control, end-to-end conversation, and enterprise support can be completed with Azure AI Language's conversational language understanding feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe conversational language understanding\n",
    "To work with conversational language understanding, you need to take into account three core concepts: utterances, entities, and intents.\n",
    "\n",
    "### Utterances\n",
    "An utterance is an example of something a user might say, and which your application must interpret. For example, when using a home automation system, a user might use the following utterances:\n",
    "- *\"Switch the fan on.\"*\n",
    "- *\"Turn on the light.\"*\n",
    "\n",
    "### Entities\n",
    "An entity is an item to which an utterance refers. For example, fan and light in the following utterances:\n",
    "- *\"Switch the fan on.\"*\n",
    "- *\"Turn on the light.\"*\n",
    "You can think of the fan and light entities as being specific instances of a general device entity.\n",
    "\n",
    "### Intents\n",
    "An intent represents the purpose, or goal, expressed in a user's utterance. For example, for both of the previously considered utterances, the intent is to turn a device on; so in your conversational language understanding application, you might define a TurnOn intent that is related to these utterances.\n",
    "\n",
    "A conversational language understanding application defines a model consisting of intents and entities. Utterances are used to train the model to identify the most likely intent and the entities to which it should be applied based on a given input. The home assistant application we've been considering might include multiple intents, like the following examples:\n",
    "\n",
    "| Intent       | Related Utterances                               | Entities                      |\n",
    "|--------------|--------------------------------------------------|-------------------------------|\n",
    "| Greeting     | \"Hello\"                                          |                               |\n",
    "|              | \"Hi\"                                             |                               |\n",
    "|              | \"Hey\"                                            |                               |\n",
    "|              | \"Good morning\"                                   |                               |\n",
    "| TurnOn       | \"Switch the fan on\"                              | fan (device)                  |\n",
    "|              | \"Turn the light on\"                              | light (device)                |\n",
    "|              | \"Turn on the light\"                              | light (device)                |\n",
    "| TurnOff      | \"Switch the fan off\"                             | fan (device)                  |\n",
    "|              | \"Turn the light off\"                             | light (device)                |\n",
    "|              | \"Turn off the light\"                             | light (device)                |\n",
    "| CheckWeather | \"What is the weather for today?\"                 | today (datetime)              |\n",
    "|              | \"Give me the weather forecast\"                   |                               |\n",
    "|              | \"What is the forecast for Paris?\"                | Paris (location)              |\n",
    "|              | \"What will the weather be like in Seattle tomorrow?\" | Seattle (location), tomorrow (datetime) |\n",
    "| None         | \"What is the meaning of life?\"                   |                               |\n",
    "|              | \"Is this thing on?\"                              |                               |\n",
    "\n",
    "\n",
    "In the table there are numerous utterances used for each of the intents. The intent should be a concise way of grouping the utterance tasks. Of special interest is the None intent. You should consider always using the None intent to help handle utterances that do not map any of the utterances you have entered. The None intent is considered a fallback, and is typically used to provide a generic response to users when their requests don't match any other intent.\n",
    "\n",
    "After defining the entities and intents with sample utterances in your conversational language understanding application, you can train a language model to predict intents and entities from user input - even if it doesn't match the sample utterances exactly. You can then use the model from a client application to retrieve predictions and respond appropriately.\n",
    "\n",
    "# Get started with conversational language understanding in Azure\n",
    "Azure AI Language's conversational language understanding feature enables you to author a language model and use it for predictions. Authoring a model involves defining entities, intents, and utterances. Generating predictions involves publishing a model so that client applications can take user input and return responses.\n",
    "\n",
    "### Azure resources for conversational language understanding\n",
    "\n",
    "To use conversational language capabilities in Azure, you need a resource in your Azure subscription. You can use the following types of resource:\n",
    "- Azure AI Language: A resource that enables you to build apps with industry-leading natural language understanding capabilities without machine learning expertise. You can use a language resource for authoring and prediction.\n",
    "- Azure AI services: A general resource that includes conversational language understanding along with many other Azure AI services. You can only use this type of resource for prediction.\n",
    "The separation of resources is useful when you want to track resource utilization for Azure AI Language use separately from client applications using all Azure AI services applications.\n",
    "\n",
    "### Authoring\n",
    "After you've created an authoring resource, you can use it to train a conversational language understanding model. To train a model, start by defining the entities and intents that your application will predict as well as utterances for each intent that can be used to train the predictive model.\n",
    "\n",
    "Conversational language understanding provides a comprehensive collection of prebuilt domains that include pre-defined intents and entities for common scenarios; which you can use as a starting point for your model. You can also create your own entities and intents.\n",
    "When you create entities and intents, you can do so in any order. You can create an intent, and select words in the sample utterances you define for it to create entities for them; or you can create the entities ahead of time and then map them to words in utterances as you're creating the intents.\n",
    "\n",
    "You can write code to define the elements of your model, but in most cases it's easiest to author your model using the Language studio - a web-based interface for creating and managing Conversational Language Understanding applications.\n",
    "\n",
    "### Training the model\n",
    "After you have defined the intents and entities in your model, and included a suitable set of sample utterances; the next step is to train the model. Training is the process of using your sample utterances to teach your model to match natural language expressions that a user might say to probable intents and entities.\n",
    "\n",
    "After training the model, you can test it by submitting text and reviewing the predicted intents. Training and testing is an iterative process. After you train your model, you test it with sample utterances to see if the intents and entities are recognized correctly. If they're not, make updates, retrain, and test again.\n",
    "\n",
    "### Predicting\n",
    "When you are satisfied with the results from the training and testing, you can publish your Conversational Language Understanding application to a prediction resource for consumption.\n",
    "Client applications can use the model by connecting to the endpoint for the prediction resource, specifying the appropriate authentication key; and submit user input to get predicted intents and entities. The predictions are returned to the client application, which can then take appropriate action based on the predicted intent."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
