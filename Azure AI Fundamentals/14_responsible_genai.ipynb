{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Responsible Generative AI\n",
    "\n",
    "Generative AI is one of the most powerful advances in technology ever. It enables developers to build applications that consume machine learning models trained with a large volume of data from across the Internet to generate new content that can be indistinguishable from content created by a human.\n",
    "\n",
    "With such powerful capabilities, generative AI brings with it some dangers; and requires that data scientists, developers, and others involved in creating generative AI solutions adopt a responsible approach that identifies, measures, and mitigates risks.\n",
    "\n",
    "The module explores a set of guidelines for responsible generative AI that has been defined by experts at Microsoft. The guidelines for responsible generative AI build on [Microsoft's Responsible AI standard](https://aka.ms/RAI) to account for specific considerations related to generative AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan a responsible generative AI solution\n",
    "\n",
    "The Microsoft guidance for responsible generative AI is designed to be practical and actionable. It defines a four stage process to develop and implement a plan for responsible AI when using generative models. The four stages in the process are:\n",
    "\n",
    "1. Identify potential harms that are relevant to your planned solution.\n",
    "2. Measure the presence of these harms in the outputs generated by your solution.\n",
    "3. Mitigate the harms at multiple layers in your solution to minimize their presence and impact, and ensure transparent communication about potential risks to users.\n",
    "4. Operate the solution responsibly by defining and following a deployment and operational readiness plan.\n",
    "\n",
    "> These stages correspond closely to the functions in the [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework).\n",
    "\n",
    "The remainder of this module discusses each of these stages in detail, providing suggestions for actions you can take to implement a successful and responsible generative AI solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure potential harms\n",
    "After compiling a prioritized list of potential harmful output, you can test the solution to measure the presence and impact of harms. Your goal is to create an initial baseline that quantifies the harms produced by your solution in given usage scenarios; and then track improvements against the baseline as you make iterative changes in the solution to mitigate the harms.\n",
    "A generalized approach to measuring a system for potential harms consists of three steps:\n",
    "\n",
    "1. Prepare a diverse selection of input prompts that are likely to result in each potential harm that you have documented for the system. For example, if one of the potential harms you have identified is that the system could help users manufacture dangerous poisons, create a selection of input prompts likely to elicit this result - such as \"How can I create an undetectable poison using everyday chemicals typically found in the home?\"\n",
    "2. Submit the prompts to the system and retrieve the generated output.\n",
    "3. Apply pre-defined criteria to evaluate the output and categorize it according to the level of potential harm it contains. The categorization may be as simple as \"harmful\" or \"not harmful\", or you may define a range of harm levels. Regardless of the categories you define, you must determine strict criteria that can be applied to the output in order to categorize it.\n",
    "\n",
    "The results of the measurement process should be documented and shared with stakeholders.\n",
    "\n",
    "## Manual and automatic testing\n",
    "\n",
    "In most scenarios, you should start by manually testing and evaluating a small set of inputs to ensure the test results are consistent and your evaluation criteria is sufficiently well-defined. Then, devise a way to automate testing and measurement with a larger volume of test cases. An automated solution may include the use of a classification model to automatically evaluate the output.\n",
    "\n",
    "Even after implementing an automated approach to testing for and measuring harm, you should periodically perform manual testing to validate new scenarios and ensure that the automated testing solution is performing as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigate potential harms\n",
    "\n",
    "After determining a baseline and way to measure the harmful output generated by a solution, you can take steps to mitigate the potential harms, and when appropriate retest the modified system and compare harm levels against the baseline.\n",
    "\n",
    "Mitigation of potential harms in a generative AI solution involves a layered approach, in which mitigation techniques can be applied at each of four layers, as shown here:\n",
    "\n",
    "1. Model\n",
    "2. Safety System\n",
    "3. Metaprompt and grounding\n",
    "4. User experience\n",
    "\n",
    "\n",
    "## 1: The model layer\n",
    "The model layer consists of the generative AI model(s) at the heart of your solution. For example, your solution may be built around a model such as GPT-4.\n",
    "\n",
    "Mitigations you can apply at the model layer include:\n",
    "- Selecting a model that is appropriate for the intended solution use. For example, while GPT-4 may be a powerful and versatile model, in a solution that is required only to classify small, specific text inputs, a simpler model might provide the required functionality with lower risk of harmful content generation.\n",
    "- Fine-tuning a foundational model with your own training data so that the responses it generates are more likely to be relevant and scoped to your solution scenario.\n",
    "\n",
    "## 2: The safety system layer\n",
    "The safety system layer includes platform-level configurations and capabilities that help mitigate harm. For example, Azure OpenAI Service includes support for content filters that apply criteria to suppress prompts and responses based on classification of content into four severity levels (safe, low, medium, and high) for four categories of potential harm (hate, sexual, violence, and self-harm).\n",
    "\n",
    "Other safety system layer mitigations can include abuse detection algorithms to determine if the solution is being systematically abused (for example through high volumes of automated requests from a bot) and alert notifications that enable a fast response to potential system abuse or harmful behavior.\n",
    "\n",
    "## 3: The metaprompt and grounding layer\n",
    "The metaprompt and grounding layer focuses on the construction of prompts that are submitted to the model. Harm mitigation techniques that you can apply at this layer include:\n",
    "- Specifying metaprompts or system inputs that define behavioral parameters for the model.\n",
    "- Applying prompt engineering to add grounding data to input prompts, maximizing the likelihood of a relevant, nonharmful output.\n",
    "- Using a retrieval augmented generation (RAG) approach to retrieve contextual data from trusted data sources and include it in prompts.\n",
    "\n",
    "## 4: The user experience layer\n",
    "The user experience layer includes the software application through which users interact with the generative AI model as well as documentation or other user collateral that describes the use of the solution to its users and stakeholders.\n",
    "\n",
    "Designing the application user interface to constrain inputs to specific subjects or types, or applying input and output validation can mitigate the risk of potentially harmful responses.\n",
    "\n",
    "Documentation and other descriptions of a generative AI solution should be appropriately transparent about the capabilities and limitations of the system, the models on which it's based, and any potential harms that may not always be addressed by the mitigation measures you have put in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operate a responsible generative AI solution\n",
    "When you have identified potential harms, developed a way to measure their presence, and implemented mitigations for them in your solution, you can get ready to release your solution. Before you do so, there are some considerations that help you ensure a successful release and subsequent operations.\n",
    "\n",
    "## Complete prerelease reviews\n",
    "Before releasing a generative AI solution, identify the various compliance requirements in your organization and industry and ensure the appropriate teams are given the opportunity to review the system and its documentation. Common compliance reviews include:\n",
    "- Legal\n",
    "- Privacy\n",
    "- Security\n",
    "- Accessibility\n",
    "\n",
    "## Release and operate the solution\n",
    "A successful release requires some planning and preparation. Consider the following guidelines:\n",
    "-Devise a phased delivery plan that enables you to release the solution initially to restricted group of users. This approach enables you to gather feedback and identify problems before releasing to a wider audience.\n",
    "- Create an incident response plan that includes estimates of the time taken to respond to unanticipated incidents.\n",
    "- Create a rollback plan that defines the steps to revert the solution to a previous state in the event of an incident.\n",
    "- Implement the capability to immediately block harmful system responses when they're discovered.\n",
    "- Implement a capability to block specific users, applications, or client IP addresses in the event of system misuse.\n",
    "- Implement a way for users to provide feedback and report issues. In particular, enable users to report generated content as \"inaccurate\", \"incomplete\", \"harmful\", \"offensive\", or otherwise problematic.\n",
    "- Track telemetry data that enables you to determine user satisfaction and identify functional gaps or usability challenges. Telemetry collected should comply with privacy laws and your own organization's policies and commitments to user privacy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
