{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a model to a batch endpoint\n",
    "Imagine you trained a model to predict the product sales. The model has been trained and tracked in Azure Machine Learning. Every month, you want to use the model to forecast the sales for the upcoming month.\n",
    "\n",
    "In many production scenarios, long-running tasks that deal with large amounts of data are performed as batch operations. In machine learning, batch inferencing is used to asynchronously apply a predictive model to multiple cases and write the results to a file or database.\n",
    "\n",
    "In Azure Machine Learning, you can implement batch inferencing solutions by deploying a model to a batch endpoint.\n",
    "\n",
    "## Understand and create batch endpoints\n",
    "To get a model to generate batch predictions, you can deploy the model to a batch endpoint.\n",
    "\n",
    "You'll learn how to use batch endpoints for asynchronous batch scoring.\n",
    "\n",
    "## Batch predictions\n",
    "To get batch predictions, you can deploy a model to an endpoint. An endpoint is an HTTPS endpoint that you can call to trigger a batch scoring job. The advantage of such an endpoint is that you can trigger the batch scoring job from another service, such as Azure Synapse Analytics or Azure Databricks. A batch endpoint allows you to integrate the batch scoring with an existing data ingestion and transformation pipeline.\n",
    "\n",
    "Whenever the endpoint is invoked, a batch scoring job is submitted to the Azure Machine Learning workspace. The job typically uses a compute cluster to score multiple inputs. The results can be stored in a datastore, connected to the Azure Machine Learning workspace.\n",
    "\n",
    "## Create a batch endpoint\n",
    "To deploy a model to a batch endpoint, you'll first have to create the batch endpoint.\n",
    "\n",
    "To create a batch endpoint, you'll use the BatchEndpoint class. Batch endpoint names need to be unique within an Azure region.\n",
    "\n",
    "To create an endpoint, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch endpoint\n",
    "endpoint = BatchEndpoint(\n",
    "    name=\"endpoint-example\",\n",
    "    description=\"A batch endpoint\",\n",
    ")\n",
    "\n",
    "ml_client.batch_endpoints.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Explore the reference documentation to [create a batch endpoint with the Python SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.batchendpoint).\n",
    "\n",
    "## Deploy a model to a batch endpoint\n",
    "You can deploy multiple models to a batch endpoint. Whenever you call the batch endpoint, which triggers a batch scoring job, the default deployment will be used unless specified otherwise.\n",
    "\n",
    "![alt text](assets/batch-details.png)\n",
    "\n",
    "## Use compute clusters for batch deployments\n",
    "The ideal compute to use for batch deployments is the Azure Machine Learning compute cluster. If you want the batch scoring job to process the new data in parallel batches, you need to provision a compute cluster with more than one maximum instances.\n",
    "\n",
    "To create a compute cluster, you can use the AMLCompute class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cpu_cluster = AmlCompute(\n",
    "    name=\"aml-cluster\",\n",
    "    type=\"amlcompute\",\n",
    "    size=\"STANDARD_DS11_V2\",\n",
    "    min_instances=0,\n",
    "    max_instances=4,\n",
    "    idle_time_before_scale_down=120,\n",
    "    tier=\"Dedicated\",\n",
    ")\n",
    "\n",
    "cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy your MLflow model to a batch endpoint\n",
    "An easy way to deploy a model to a batch endpoint is to use an MLflow model. Azure Machine Learning will automatically generate the scoring script and environment for MLflow models.\n",
    "\n",
    "To deploy an MLflow model, you need to have created an endpoint. Then you can deploy the model to the endpoint.\n",
    "\n",
    "## Register an MLflow model\n",
    "To avoid needed a scoring script and environment, an MLflow model needs to be registered in the Azure Machine Learning workspace before you can deploy it to a batch endpoint.\n",
    "\n",
    "To register an MLflow model, you'll use the Model class, while specifying the model type to be MLFLOW_MODEL. To register the model with the Python SDK, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "model_name = 'mlflow-model'\n",
    "model = ml_client.models.create_or_update(\n",
    "    Model(name=model_name, path='./model', type=AssetTypes.MLFLOW_MODEL)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're taking the model files from a local path. The files are all stored in a local folder called model. The folder must include the MLmodel file, which describes how the model can be loaded and used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an MLflow model to an endpoint\n",
    "To deploy an MLflow model to a batch endpoint, you'll use the BatchDeployment class.\n",
    "\n",
    "When you deploy a model, you'll need to specify how you want the batch scoring job to behave. The advantage of using a compute cluster to run the scoring script (which is automatically generated by Azure Machine Learning), is that you can run the scoring script on separate instances in parallel.\n",
    "\n",
    "When you configure the model deployment, you can specify:\n",
    "\n",
    "- instance_count: Count of compute nodes to use for generating predictions.\n",
    "- max_concurrency_per_instance: Maximum number of parallel scoring script runs per compute node.\n",
    "- mini_batch_size: Number of files passed per scoring script run.\n",
    "- output_action: What to do with the predictions: summary_only or append_row.\n",
    "- output_file_name: File to which predictions will be appended, if you choose append_row for output_action.\n",
    "\n",
    ">Explore the reference documentation to [create a batch deployment with the Python SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.batchdeployment).\n",
    "\n",
    "To deploy an MLflow model to a batch endpoint, you can use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import BatchDeployment, BatchRetrySettings\n",
    "from azure.ai.ml.constants import BatchDeploymentOutputAction\n",
    "\n",
    "deployment = BatchDeployment(\n",
    "    name=\"forecast-mlflow\",\n",
    "    description=\"A sales forecaster\",\n",
    "    endpoint_name=endpoint.name,\n",
    "    model=model,\n",
    "    compute=\"aml-cluster\",\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=2,\n",
    "    output_action=BatchDeploymentOutputAction.APPEND_ROW,\n",
    "    output_file_name=\"predictions.csv\",\n",
    "    retry_settings=BatchRetrySettings(max_retries=3, timeout=300),\n",
    "    logging_level=\"info\",\n",
    ")\n",
    "ml_client.batch_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a custom model to a batch endpoint\n",
    "If you want to deploy a model to a batch endpoint without using the MLflow model format, you need to create the scoring script and environment.\n",
    "\n",
    "To deploy a model, you must have already created an endpoint. Then you can deploy the model to the endpoint.\n",
    "\n",
    "## Create the scoring script\n",
    "The scoring script is a file that reads the new data, loads the model, and performs the scoring.\n",
    "The scoring script must include two functions:\n",
    "- init(): Called once at the beginning of the process, so use for any costly or common preparation like loading the model.\n",
    "- run(): Called for each mini batch to perform the scoring.\n",
    "\n",
    "The run() method should return a pandas DataFrame or an array/list.\n",
    "\n",
    "A scoring script may look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "\n",
    "    # get the path to the registered model file and load it\n",
    "    model_path = os.path.join(os.environ[\"AZUREML_MODEL_DIR\"], \"model\")\n",
    "    model = mlflow.pyfunc.load(model_path)\n",
    "\n",
    "\n",
    "def run(mini_batch):\n",
    "    print(f\"run method start: {__file__}, run({len(mini_batch)} files)\")\n",
    "    resultList = []\n",
    "\n",
    "    for file_path in mini_batch:\n",
    "        data = pd.read_csv(file_path)\n",
    "        pred = model.predict(data)\n",
    "\n",
    "        df = pd.DataFrame(pred, columns=[\"predictions\"])\n",
    "        df[\"file\"] = os.path.basename(file_path)\n",
    "        resultList.extend(df.values)\n",
    "\n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some things to note from the example script:\n",
    "- AZUREML_MODEL_DIR is an environment variable that you can use to locate the files associated with the model.\n",
    "- Use global variable to make any assets available that are needed to score the new data, like the loaded model.\n",
    "- The size of the mini_batch is defined in the deployment configuration. If the files in the mini batch are too large to be processed, you need to split the files into smaller files.\n",
    "- By default, the predictions will be written to one single file.\n",
    "\n",
    ">Learn more about how to [author scoring scripts for batch deployments](https://learn.microsoft.com/en-us/azure/machine-learning/batch-inference/how-to-batch-scoring-script).\n",
    "\n",
    "## Create an environment\n",
    "\n",
    "Your deployment requires an execution environment in which to run the scoring script. Any dependency your code requires should be included in the environment.\n",
    "\n",
    "You can create an environment with a Docker image with Conda dependencies, or with a Dockerfile.\n",
    "\n",
    "You'll also need to add the library azureml-core as it is required for batch deployments to work.\n",
    "\n",
    "To create an environment using a base Docker image, you can define the Conda dependencies in a conda.yaml file:\n",
    "\n",
    "```yaml\n",
    "\n",
    "name: basic-env-cpu\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pandas\n",
    "  - pip\n",
    "  - pip:\n",
    "      - azureml-core\n",
    "      - mlflow\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Then, to create the environment, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "    conda_file=\"./src/conda-env.yml\",\n",
    "    name=\"deployment-environment\",\n",
    "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and create the deployment\n",
    "Finally, you can configure and create the deployment with the BatchDeployment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import BatchDeployment, BatchRetrySettings\n",
    "from azure.ai.ml.constants import BatchDeploymentOutputAction\n",
    "\n",
    "deployment = BatchDeployment(\n",
    "    name=\"forecast-mlflow\",\n",
    "    description=\"A sales forecaster\",\n",
    "    endpoint_name=endpoint.name,\n",
    "    model=model,\n",
    "    compute=\"aml-cluster\",\n",
    "    code_path=\"./code\",\n",
    "    scoring_script=\"score.py\",\n",
    "    environment=env,\n",
    "    instance_count=2,\n",
    "    max_concurrency_per_instance=2,\n",
    "    mini_batch_size=2,\n",
    "    output_action=BatchDeploymentOutputAction.APPEND_ROW,\n",
    "    output_file_name=\"predictions.csv\",\n",
    "    retry_settings=BatchRetrySettings(max_retries=3, timeout=300),\n",
    "    logging_level=\"info\",\n",
    ")\n",
    "ml_client.batch_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke and troubleshoot batch endpoints\n",
    "When you invoke a batch endpoint, you trigger an Azure Machine Learning pipeline job. The job will expect an input parameter pointing to the data set you want to score.\n",
    "\n",
    "## Trigger the batch scoring job\n",
    "To prepare data for batch predictions, you can register a folder as a data asset in the Azure Machine Learning workspace.\n",
    "\n",
    "You can then use the registered data asset as input when invoking the batch endpoint with the Python SDK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "input = Input(type=AssetTypes.URI_FOLDER, path=\"azureml:new-data:1\")\n",
    "\n",
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint.name, \n",
    "    input=input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the run of the pipeline job in the Azure Machine Learning studio. All jobs that are triggered by invoking the batch endpoint will show in the Jobs tab of the batch endpoint.\n",
    "\n",
    "![alt text](assets/batch-jobs.png)\n",
    "\n",
    "The predictions will be stored in the default datastore.\n",
    "\n",
    "## Troubleshoot a batch scoring job\n",
    "The batch scoring job runs as a pipeline job. If you want to troubleshoot the pipeline job, you can review its details and the outputs and logs of the pipeline job itself.\n",
    "\n",
    "![alt text](assets/child-job.png)\n",
    "\n",
    "If you want to troubleshoot the scoring script, you can select the child job and review its outputs and logs.\n",
    "\n",
    "Navigate to the Outputs + logs tab. The logs/user/ folder contains three files that will help you troubleshoot:\n",
    "- job_error.txt: Summarize the errors in your script.\n",
    "- job_progress_overview.txt: Provides high-level information about the number of mini-batches processed so far.\n",
    "- job_result.txt: Shows errors in calling the init() and run() function in the scoring script.\n",
    "\n",
    "\n",
    "![alt text](assets/child-output.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
