{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track model training with MLflow in jobs\n",
    "Scripts are ideal when you want to run machine learning workloads in production environments. Imagine you're a data scientist who has developed a machine learning model to predict diabetes. The model is performing as expected and you created a training script. The script is used to retrain the model every month when new data has been collected.\n",
    "\n",
    "You'll want to monitor the model's performance over time. You want to understand whether the new data every month benefits the model. Next to tracking models that are trained in notebooks, you can also use MLflow to track models in scripts.\n",
    "\n",
    "MLflow is an open-source platform that helps you to track model metrics and artifacts across platforms and is integrated with Azure Machine Learning.\n",
    "\n",
    "When you use MLflow together with Azure Machine Learning, you can run training scripts locally or in the cloud. You can review model metrics and artifacts in the Azure Machine Learning workspace to compare runs and decide on next steps.\n",
    "\n",
    "In this module, you learn how to:\n",
    "- Use MLflow when you run a script as a job.\n",
    "- Review metrics, parameters, artifacts, and models from a run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track metrics with MLflow\n",
    "When you train a model with a script, you can include MLflow in the scripts to track any parameters, metrics, and artifacts. When you run the script as a job in Azure Machine Learning, you're able to review all input parameters and outputs for each run.\n",
    "\n",
    "## Understand MLflow\n",
    "MLflow is an open-source platform, designed to manage the complete machine learning lifecycle. As it's open source, it can be used when training models on different platforms. Here, we explore how we can integrate MLflow with Azure Machine Learning jobs.\n",
    "\n",
    "There are two options to track machine learning jobs with MLflow:\n",
    "- Enable autologging using mlflow.autolog()\n",
    "- Use logging functions to track custom metrics using mlflow.log_*\n",
    "\n",
    "Before you can use either of these options, you need to set up the environment to use MLflow.\n",
    "\n",
    "## Include MLflow in the environment\n",
    "To use MLflow during training job, the mlflow and azureml-mlflow pip packages need to be installed on the compute executing the script. Therefore, you need to include these two packages in the environment. You can create an environment by referring to a YAML file that describes the Conda environment. As part of the Conda environment, you can include these two packages.\n",
    "\n",
    "For example, in this custom environment mlflow and azureml-mlflow are installed using pip:\n",
    "\n",
    "```yaml\n",
    "name: mlflow-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "    - numpy\n",
    "    - pandas\n",
    "    - scikit-learn\n",
    "    - matplotlib\n",
    "    - mlflow\n",
    "    - azureml-mlflow\n",
    "```\n",
    "\n",
    "Once the environment is defined and registered, make sure to refer to it when submitting a job.\n",
    "\n",
    "## Enable autologging\n",
    "When working with one of the common libraries for machine learning, you can enable autologging in MLflow. Autologging logs parameters, metrics, and model artifacts without anyone needing to specify what needs to be logged.\n",
    "\n",
    "Autologging is supported for the following libraries:\n",
    "\n",
    "- Scikit-learn\n",
    "- TensorFlow and Keras\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- Spark\n",
    "- Fastai\n",
    "- Pytorch\n",
    "\n",
    "To enable autologging, add the following code to your training script:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "mlflow.autolog()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metrics with MLflow\n",
    "In your training script, you can decide whatever custom metric you want to log with MLflow.\n",
    "\n",
    "Depending on the type of value you want to log, use the MLflow command to store the metric with the experiment run:\n",
    "- mlflow.log_param(): Log single key-value parameter. Use this function for an input parameter you want to log.\n",
    "- mlflow.log_metric(): Log single key-value metric. Value must be a number. Use this function for any output you want to store with the run.\n",
    "- mlflow.log_artifact(): Log a file. Use this function for any plot you want to log, save as image file first.\n",
    "\n",
    "To add MLflow to an existing training script, you can add the following code:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "reg_rate = 0.1\n",
    "mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "```\n",
    "\n",
    ">For a complete overview of how to use MLflow Tracking, read the [MLflow documentation](https://www.mlflow.org/docs/latest/tracking.html).\n",
    "\n",
    "## Submit the job\n",
    "Finally, you need to submit the training script as a job in Azure Machine Learning. When you use MLflow in a training script and run it as a job, all tracked parameters, metrics, and artifacts are stored with the job run.\n",
    "\n",
    "You configure the job as usual. You only need to make sure that the environment you refer to in the job includes the necessary packages, and the script describes which metrics you want to log.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View metrics and evaluate models\n",
    "After you've trained and tracked models with MLflow in Azure Machine Learning, you can explore the metrics and evaluate your models.\n",
    "- Review metrics in the Azure Machine Learning studio.\n",
    "- Retrieve runs and metrics with MLflow.\n",
    "\n",
    ">Azure Machine Learning uses the concept of jobs when you run a script. Multiple job runs in Azure Machine Learning can be grouped within one experiment. MLflow uses a similar syntax where each script is a run, which is part of an experiment.\n",
    "\n",
    "## View the metrics in the Azure Machine Learning studio\n",
    "When your job is completed, you can review the logged parameters, metrics, and artifacts in the Azure Machine Learning studio.\n",
    "\n",
    "When you review job runs in the Azure Machine Learning studio, you'll explore a job run's metrics, which is part of an experiment.\n",
    "\n",
    "To view the metrics through an intuitive user interface, you can:\n",
    "1. Open the Studio by navigating to https://ml.azure.com.\n",
    "2. Find your experiment run and open it to view its details.\n",
    "3. In the Details tab, all logged parameters are shown under Params.\n",
    "4. Select the Metrics tab and select the metric you want to explore.\n",
    "5. Any plots that are logged as artifacts can be found under Images.\n",
    "6. The model assets that can be used to register and deploy the model are stored in the models folder under Outputs + logs.\n",
    "\n",
    ">Read the documentation to learn more on [how to track models with MLflow](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-mlflow).\n",
    "\n",
    "## Retrieve metrics with MLflow in a notebook\n",
    "When you run a training script as a job in Azure Machine Learning, and track your model training with MLflow, you can query the runs in a notebook by using MLflow. Using MLflow in a notebook gives you more control over which runs you want to retrieve to compare.\n",
    "\n",
    "When using MLflow to query your runs, you'll refer to experiments and runs.\n",
    "\n",
    "## Search all the experiments\n",
    "\n",
    "You can get all the active experiments in the workspace using MLFlow:\n",
    "\n",
    "```python\n",
    "experiments = mlflow.search_experiments(max_results=2)\n",
    "for exp in experiments:\n",
    "    print(exp.name)\n",
    "```\n",
    "\n",
    "If you want to retrieve archived experiments too, then include the option ViewType.ALL:\n",
    "\n",
    "```python\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "experiments = mlflow.search_experiments(view_type=ViewType.ALL)\n",
    "for exp in experiments:\n",
    "    print(exp.name)\n",
    "```\n",
    "\n",
    "To retrieve a specific experiment, you can run:\n",
    "\n",
    "```python\n",
    "exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(exp)\n",
    "```\n",
    "\n",
    ">Explore the documentation on how to [search experiments with MLflow](https://mlflow.org/docs/latest/search-experiments.html)\n",
    "\n",
    "## Retrieve runs\n",
    "\n",
    "MLflow allows you to search for runs inside of any experiment. You need either the experiment ID or the experiment name.\n",
    "For example, when you want to retrieve the metrics of a specific run:\n",
    "\n",
    "```python\n",
    "mlflow.search_runs(exp.experiment_id)\n",
    "```\n",
    "\n",
    "You can search runs across multiple experiments if necessary. Searching across experiments may be useful in case you want to compare runs of the same model when it's being logged in different experiments (by different people or different project iterations).\n",
    "\n",
    "You can use `search_all_experiments=True` if you want to search across all the experiments in the workspace.\n",
    "\n",
    "By default, experiments are ordered descending by start_time, which is the time the experiment was queued in Azure Machine Learning. However, you can change this default by using the parameter order_by.\n",
    "\n",
    "For example, if you want to sort by start time and only show the last two results:\n",
    "\n",
    "```python\n",
    "mlflow.search_runs(exp.experiment_id, order_by=[\"start_time DESC\"], max_results=2)\n",
    "```\n",
    "\n",
    "You can also look for a run with a specific combination in the hyperparameters:\n",
    "\n",
    "```python\n",
    "mlflow.search_runs(\n",
    "    exp.experiment_id, filter_string=\"params.num_boost_round='100'\", max_results=2\n",
    ")\n",
    "```\n",
    "\n",
    ">[Explore the documentation on how to search runs with MLflow](https://mlflow.org/docs/latest/search-runs.html)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
