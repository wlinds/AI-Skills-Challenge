{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipelines in Azure Machine Learning\n",
    "In Azure Machine Learning, you can experiment in notebooks and train (and retrain) machine learning models by running scripts as jobs.\n",
    "\n",
    "In an enterprise data science process, you'll want to separate the overall process into individual tasks. You can group tasks together as pipelines. Pipelines are key to implementing an effective Machine Learning Operations (MLOps) solution in Azure.\n",
    "\n",
    "You'll learn how to create components of individual tasks, making it easier to reuse and share code. You'll then combine components into an Azure Machine Learning pipeline, which you'll run as a pipeline job.\n",
    "\n",
    ">The term pipeline is used extensively across various domains, including machine learning and software engineering. In Azure Machine Learning, a pipeline contains steps related to the training of a machine learning model. In Azure DevOps or GitHub, a pipeline can refer to a build or release pipelines, which perform the build and configuration tasks required to deliver software. In Azure Synapse Analytics, a pipeline is used to define the data ingestion and transformation process. The focus of this module is on Azure Machine Learning pipelines. However, bear in mind that it's possible to have pipelines across services interact with each other. For example, an Azure DevOps or Azure Synapse Analytics pipeline can trigger an Azure Machine Learning pipeline.\n",
    "\n",
    "Learn more about MLOps in relation to Azure Machine Learning with an [introduction to machine learning operations](https://learn.microsoft.com/en-us/training/paths/introduction-machine-learn-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create components\n",
    "Components allow you to create reusable scripts that can easily be shared across users within the same Azure Machine Learning workspace. You can also use components to build an Azure Machine Learning pipeline.\n",
    "\n",
    "There are two main reasons why you'd use components:\n",
    "- To build a pipeline.\n",
    "- To share ready-to-go code.\n",
    "\n",
    "You'll want to create components when you're preparing your code for scale. When you're done with experimenting and developing, and ready to move your model to production.\n",
    "\n",
    "Within Azure Machine Learning, you can create a component to store code (in your preferred language) within the workspace. Ideally, you design a component to perform a specific action that is relevant to your machine learning workflow.\n",
    "\n",
    "For example, a component may consist of a Python script that normalizes your data, trains a machine learning model, or evaluates a model.\n",
    "\n",
    "Components can be easily shared to other Azure Machine Learning users, who can reuse components in their own Azure Machine Learning pipelines.\n",
    "\n",
    "![alt text](assets/01-01-components.png)\n",
    "\n",
    "A component consists of three parts:\n",
    "- Metadata: Includes the component's name, version, etc.\n",
    "- Interface: Includes the expected input parameters (like a dataset or hyperparameter) and expected output (like metrics and artifacts).\n",
    "- Command, code and environment: Specifies how to run the code.\n",
    "\n",
    "To create a component, you need two files:\n",
    "- A script that contains the workflow you want to execute.\n",
    "- A YAML file to define the metadata, interface, and command, code, and environment of the component.\n",
    "\n",
    "You can create the YAML file, or use the `command_component()` function as a decorator to create the YAML file.\n",
    "\n",
    ">Here, we'll focus on creating a YAML file to create a component. Alternatively, learn more about [how to create components using command_component()](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python).\n",
    "\n",
    "\n",
    "For example, you may have a Python script `prep.py` that prepares the data by removing missing values and normalizing the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# setup arg parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# add arguments\n",
    "parser.add_argument(\"--input_data\", dest='input_data',\n",
    "                    type=str)\n",
    "parser.add_argument(\"--output_data\", dest='output_data',\n",
    "                    type=str)\n",
    "\n",
    "# parse args\n",
    "args = parser.parse_args()\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(args.input_data)\n",
    "\n",
    "# remove missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# normalize the data    \n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['feature1','feature2','feature3','feature4']\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# save the data as a csv\n",
    "output_df = df.to_csv(\n",
    "    (Path(args.output_data) / \"prepped-data.csv\"), \n",
    "    index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a component for the `prep.py` script, you'll need a YAML file `prep.yml`:\n",
    "\n",
    "```yaml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: prep_data\n",
    "display_name: Prepare training data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data: \n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_file\n",
    "code: ./src\n",
    "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
    "command: >-\n",
    "  python prep.py \n",
    "  --input_data ${{inputs.input_data}}\n",
    "  --output_data ${{outputs.output_data}}\n",
    "  ```\n",
    "\n",
    "  Notice that the YAML file refers to the `prep.py` script, which is stored in the `src` folder. You can load the component with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "parent_dir = \"\"\n",
    "\n",
    "loaded_component_prep = load_component(source=parent_dir + \"./prep.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you've loaded the component, you can use it in a pipeline or register the component.\n",
    "\n",
    "## Register a component\n",
    "To use components in a pipeline, you'll need the script and the YAML file. To make the components accessible to other users in the workspace, you can also register components to the Azure Machine Learning workspace.\n",
    "\n",
    "You can register a component with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ml_client.components.create_or_update(prepare_data_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pipeline\n",
    "In Azure Machine Learning, a pipeline is a workflow of machine learning tasks in which each task is defined as a component.\n",
    "\n",
    "Components can be arranged sequentially or in parallel, enabling you to build sophisticated flow logic to orchestrate machine learning operations. Each component can be run on a specific compute target, making it possible to combine different types of processing as required to achieve an overall goal.\n",
    "\n",
    "A pipeline can be executed as a process by running the pipeline as a pipeline job. Each component is executed as a child job as part of the overall pipeline job.\n",
    "\n",
    "## Build a pipeline\n",
    "An Azure Machine Learning pipeline is defined in a YAML file. The YAML file includes the pipeline job name, inputs, outputs, and settings.\n",
    "You can create the YAML file, or use the `@pipeline()` function to create the YAML file.\n",
    "\n",
    ">Review the [reference documentation for the @pipeline() function](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.dsl).\n",
    "\n",
    "For example, if you want to build a pipeline that first prepares the data, and then trains the model, you can use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def pipeline_function_name(pipeline_job_input):\n",
    "    prep_data = loaded_component_prep(input_data=pipeline_job_input)\n",
    "    train_model = loaded_component_train(training_data=prep_data.outputs.output_data)\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_transformed_data\": prep_data.outputs.output_data,\n",
    "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass a registered data asset as the pipeline job input, you can call the function you created with the data asset as input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "pipeline_job = pipeline_function_name(\n",
    "    Input(type=AssetTypes.URI_FILE, \n",
    "    path=\"azureml:data:1\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@pipeline()` function builds a pipeline consisting of two sequential steps, represented by the two loaded components.\n",
    "\n",
    "To understand the pipeline built in the example, let's explore it step by step:\n",
    "\n",
    "1. The pipeline is built by defining the function pipeline_function_name.\n",
    "2. The pipeline function expects pipeline_job_input as the overall pipeline input.\n",
    "3. The first pipeline step requires a value for the input parameter input_data. The value for the input will be the value of pipeline_job_input.\n",
    "4. The first pipeline step is defined by the loaded component for prep_data.\n",
    "5. The value of the output_data of the first pipeline step is used for the expected input training_data of the second pipeline step.\n",
    "6. The second pipeline step is defined by the loaded component for train_model and results in a trained model referred to by model_output.\n",
    "7. Pipeline outputs are defined by returning variables from the pipeline function. There are two outputs:\n",
    "    - pipeline_job_transformed_data with the value of prep_data.outputs.output_data\n",
    "    - pipeline_job_trained_model with the value of train_model.outputs.model_output\n",
    "\n",
    "![alt text](assets/pipeline-overview.png)\n",
    "\n",
    "The result of running the @pipeline() function is a YAML file that you can review by printing the pipeline_job object you created when calling the function:\n",
    "\n",
    "```py\n",
    "print(pipeline_job)\n",
    "```\n",
    "\n",
    "The output will be formatted as a YAML file, which includes the configuration of the pipeline and its components. Some parameters included in the YAML file are shown in the following example.\n",
    "\n",
    "```yaml\n",
    "display_name: pipeline_function_name\n",
    "type: pipeline\n",
    "inputs:\n",
    "  pipeline_job_input:\n",
    "    type: uri_file\n",
    "    path: azureml:data:1\n",
    "outputs:\n",
    "  pipeline_job_transformed_data: null\n",
    "  pipeline_job_trained_model: null\n",
    "jobs:\n",
    "  prep_data:\n",
    "    type: command\n",
    "    inputs:\n",
    "      input_data:\n",
    "        path: ${{parent.inputs.pipeline_job_input}}\n",
    "    outputs:\n",
    "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
    "  train_model:\n",
    "    type: command\n",
    "    inputs:\n",
    "      input_data:\n",
    "        path: ${{parent.outputs.pipeline_job_transformed_data}}\n",
    "    outputs:\n",
    "      output_model: ${{parent.outputs.pipeline_job_trained_model}}\n",
    "tags: {}\n",
    "properties: {}\n",
    "settings: {}\n",
    "```\n",
    "\n",
    ">Learn more about [the pipeline job YAML schema to explore which parameters are included when building a component-based pipeline](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-job-pipeline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a pipeline job\n",
    "When you've built a component-based pipeline in Azure Machine Learning, you can run the workflow as a pipeline job.\n",
    "\n",
    "## Configure a pipeline job\n",
    "A pipeline is defined in a YAML file, which you can also create using the `@pipeline()` function. After you've used the function, you can edit the pipeline configurations by specifying which parameters you want to change and the new value.\n",
    "\n",
    "For example, you may want to change the output mode for the pipeline job outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the output mode\n",
    "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
    "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you may want to set the default pipeline compute. When a compute isn't specified for a component, it will use the default compute instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"aml-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to change the default datastore to where all outputs will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pipeline level datastore\n",
    "pipeline_job.settings.default_datastore = \"workspaceblobstore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review your pipeline configuration, you can print the pipeline job object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a pipeline job\n",
    "When you've configured the pipeline, you're ready to run the workflow as a pipeline job.\n",
    "\n",
    "To submit the pipeline job, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_job\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you submit a pipeline job, a new job will be created in the Azure Machine Learning workspace. A pipeline job also contains child jobs, which represent the execution of the individual components. The Azure Machine Learning studio creates a graphical representation of your pipeline. You can expand the Job overview to explore the pipeline parameters, outputs, and child jobs:\n",
    "\n",
    "![alt text](assets/pipeline-output.png)\n",
    "\n",
    "To troubleshoot a failed pipeline, you can check the outputs and logs of the pipeline job and its child jobs.\n",
    "\n",
    "- If there's an issue with the configuration of the pipeline itself, you'll find more information in the outputs and logs of the pipeline job.\n",
    "- If there's an issue with the configuration of a component, you'll find more information in the outputs and logs of the child job of the failed component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule a pipeline job\n",
    "A pipeline is ideal if you want to get your model ready for production. Pipelines are especially useful for automating the retraining of a machine learning model. To automate the retraining of a model, you can schedule a pipeline.\n",
    "\n",
    "To schedule a pipeline job, you'll use the JobSchedule class to associate a schedule to a pipeline job.\n",
    "\n",
    "There are various ways to create a schedule. A simple approach is to create a time-based schedule using the RecurrenceTrigger class with the following parameters:\n",
    "- frequency: Unit of time to describe how often the schedule fires. Value can be either minute, hour, day, week, or month.\n",
    "- interval: Number of frequency units to describe how often the schedule fires. Value needs to be an integer.\n",
    "\n",
    "To create a schedule that fires every minute, run the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import RecurrenceTrigger\n",
    "\n",
    "schedule_name = \"run_every_minute\"\n",
    "\n",
    "recurrence_trigger = RecurrenceTrigger(\n",
    "    frequency=\"minute\",\n",
    "    interval=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To schedule a pipeline, you'll need pipeline_job to represent the pipeline you've built:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import JobSchedule\n",
    "\n",
    "job_schedule = JobSchedule(\n",
    "    name=schedule_name, trigger=recurrence_trigger, create_job=pipeline_job\n",
    ")\n",
    "\n",
    "job_schedule = ml_client.schedules.begin_create_or_update(\n",
    "    schedule=job_schedule\n",
    ").result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display names of the jobs triggered by the schedule will be prefixed with the name of your schedule. You can review the jobs in the Azure Machine Learning studio:\n",
    "\n",
    "![alt text](assets/scheduled-jobs.png)\n",
    "\n",
    "To delete a schedule, you first need to disable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.schedules.begin_disable(name=schedule_name).result()\n",
    "ml_client.schedules.begin_delete(name=schedule_name).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Learn more about [the schedules you can create to trigger pipeline jobs in Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-schedule-pipeline-job?tabs=python%3Fazure-portal%3Dtrue). Or, explore an [example notebook to learn how to work with schedules](https://github.com/Azure/azureml-examples/blob/main/sdk/python/schedules/job-schedule.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
