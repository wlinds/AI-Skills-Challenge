{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a model to a managed online endpoint\n",
    "Imagine you trained a model to recommend restaurants. The model has been trained and tracked in Azure Machine Learning. You want to use the model in your application where consumers can browse restaurants in their area. Each time a consumer selects a restaurant in the application, you want the model to recommend other restaurants that might also be of interest to the consumer to improve the user experience.\n",
    "\n",
    "Whenever you train a model, you ultimately will want to consume the model. You want to use the trained model to predict labels for new data on which the model hasn't been trained.\n",
    "\n",
    "To consume the model, you need to deploy it. One way to deploy a model is to integrate it with a service that allows applications to request instant, or real-time, predictions for individual or small sets of data points.\n",
    "\n",
    "In Azure Machine Learning, you can use online endpoints to deploy and consume your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore managed online endpoints\n",
    "To make a machine learning model available for other applications, you can deploy the model to a managed online endpoint.\n",
    "\n",
    "You'll learn how to use managed online endpoints for real-time predictions.\n",
    "\n",
    "## Real-time predictions\n",
    "To get real-time predictions, you can deploy a model to an endpoint. An endpoint is an HTTPS endpoint to which you can send data, and which will return a response (almost) immediately.\n",
    "\n",
    "Any data you send to the endpoint will serve as the input for the scoring script hosted on the endpoint. The scoring script loads the trained model to predict the label for the new input data, which is also called inferencing. The label is then part of the output that's returned.\n",
    "\n",
    "## Managed online endpoint\n",
    "Within Azure Machine Learning, there are two types of online endpoints:\n",
    "- Managed online endpoints: Azure Machine Learning manages all the underlying infrastructure.\n",
    "- Kubernetes online endpoints: Users manage the Kubernetes cluster which provides the necessary infrastructure.\n",
    "\n",
    "As a data scientist, you may prefer to work with managed online endpoints to test whether your model works as expected when deployed. If a Kubernetes online endpoint is required for control and scalability, it'll likely be managed by other teams.\n",
    "\n",
    "If you're using a managed online endpoint, you only need to specify the virtual machine (VM) type and scaling settings. Everything else, such as provisioning compute power and updating the host operating system (OS) is done for you automatically.\n",
    "\n",
    "## Deploy your model\n",
    "After you create an endpoint in the Azure Machine Learning workspace, you can deploy a model to that endpoint. To deploy your model to a managed online endpoint, you need to specify four things:\n",
    "\n",
    "- Model assets like the model pickle file, or a registered model in the Azure Machine Learning workspace.\n",
    "- Scoring script that loads the model.\n",
    "- Environment which lists all necessary packages that need to be installed on the compute of the endpoint.\n",
    "- Compute configuration including the needed compute size and scale settings to ensure you can handle the amount of requests the endpoint will receive.\n",
    "\n",
    ">When you deploy MLFlow models to an online endpoint, you don't need to provide a scoring script and environment, as both are automatically generated.\n",
    "\n",
    "All of these elements are defined in the deployment. The deployment is essentially a set of resources needed to host the model that performs the actual inferencing.\n",
    "\n",
    "## Blue/green deployment\n",
    "\n",
    "One endpoint can have multiple deployments. One approach is the blue/green deployment.\n",
    "\n",
    "Let's take the example of the restaurant recommender model. After experimentation, you select the best performing model. You use the blue deployment for this first version of the model. As new data is collected, the model can be retrained, and a new version is registered in the Azure Machine Learning workspace. To test the new model, you can use the green deployment for the second version of the model.\n",
    "\n",
    "Both versions of the model are deployed to the same endpoint, which is integrated with the application. Within the application, a user selects a restaurant, sending a request to the endpoint to get new real-time recommendations of other restaurants the user may like.\n",
    "\n",
    "When a request is sent to the endpoint, 90% of the traffic can go to the blue deployment*, and 10% of the traffic can go to the green deployment. With two versions of the model deployed on the same endpoint, you can easily test the model.\n",
    "\n",
    "After testing, you can also seamlessly transition to the new version of the model by redirecting 90% of the traffic to the green deployment. If it turns out that the new version doesn't perform better, you can easily roll back to the first version of the model by redirecting most of the traffic back to the blue deployment.\n",
    "\n",
    "Blue/green deployment allows for multiple models to be deployed to an endpoint. You can decide how much traffic to forward to each deployed model. This way, you can switch to a new version of the model without interrupting service to the consumer.\n",
    "\n",
    ">Learn more about [safe rollout for online endpoints](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-safely-rollout-online-endpoints).\n",
    "\n",
    "## Create an endpoint\n",
    "To create an online endpoint, you'll use the ManagedOnlineEndpoint class, which requires the following parameters:\n",
    "- name: Name of the endpoint. Must be unique in the Azure region.\n",
    "- auth_mode: Use key for key-based authentication. Use aml_token for Azure \n",
    "\n",
    "Machine Learning token-based authentication.\n",
    "\n",
    "To create an endpoint, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=\"endpoint-example\",\n",
    "    description=\"Online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Explore the reference documentation to [create a managed online endpoint with the Python SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlineendpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy your MLflow model to a managed online endpoint\n",
    "The easiest way to deploy a model to an online endpoint is to use an MLflow model and deploy it to a managed online endpoint. Azure Machine Learning will automatically generate the scoring script and environment for MLflow models.\n",
    "\n",
    "To deploy an MLflow model, you need to have created an endpoint. Then you can deploy the model to the endpoint.\n",
    "\n",
    "## Deploy an MLflow model to an endpoint\n",
    "When you deploy an MLflow model to a managed online endpoint, you donÂ´t need to have the scoring script and environment.\n",
    "\n",
    "To deploy an MLflow model, you must have model files stored on a local path or with a registered model. You can log model files when training a model by using MLflow tracking.\n",
    "\n",
    "In this example, we're taking the model files from a local path. The files are all stored in a local folder called model. The folder must include the MLmodel file, which describes how the model can be loaded and used.\n",
    "\n",
    ">Learn more about [the MLmodel format](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow-models#the-mlmodel-format?azure-portal=true).\n",
    "\n",
    "Next to the model, you also need to specify the compute configuration for the deployment:\n",
    "\n",
    "- instance_type: Virtual machine (VM) size to use. [Review the list of supported sizes](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
    "- instance_count: Number of instances to use.\n",
    "\n",
    "To deploy (and automatically register) the model, run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model, ManagedOnlineDeployment\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# create a blue deployment\n",
    "model = Model(\n",
    "    path=\"./model\",\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    "    description=\"my sample mlflow model\",\n",
    ")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=\"endpoint-example\",\n",
    "    model=model,\n",
    "    instance_type=\"Standard_F4s_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Explore the reference documentation to [create a managed online deployment with the Python SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlinedeployment).\n",
    "\n",
    "Since only one model is deployed to the endpoint, you want this model to take 100% of the traffic. When you deploy multiple models to the same endpoint, you can distribute the traffic among the deployed models.\n",
    "\n",
    "To route traffic to a specific deployment, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the endpoint and all associated deployments, run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=\"endpoint-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose to deploy a model to a managed online endpoint without using the MLflow model format. To deploy a model, you'll need to create the scoring script and define the environment necessary during inferencing.\n",
    "\n",
    "To deploy a model, you need to have created an endpoint. Then you can deploy the model to the endpoint.\n",
    "\n",
    "To deploy a model, you must have:\n",
    "- Model files stored on local path or registered model.\n",
    "- A scoring script.\n",
    "- An execution environment.\n",
    "The model files can be logged and stored when you train a model.\n",
    "\n",
    "## Create the scoring script\n",
    "\n",
    "The scoring script needs to include two functions:\n",
    "- init(): Called when the service is initialized.\n",
    "- run(): Called when new data is submitted to the service.\n",
    "\n",
    "The init function is called when the deployment is created or updated, to load and cache the model from the model registry. The run function is called for every time the endpoint is invoked, to generate predictions from the input data. The following example Python script shows this pattern:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# called when the deployment is created or updated\n",
    "def init():\n",
    "    global model\n",
    "    # get the path to the registered model file and load it\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# called when a request is received\n",
    "def run(raw_data):\n",
    "    # get the input data as a numpy array\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # get a prediction from the model\n",
    "    predictions = model.predict(data)\n",
    "    # return the predictions as any JSON serializable format\n",
    "    return predictions.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment\n",
    "Your deployment requires an execution environment in which to run the scoring script.\n",
    "\n",
    "You can create an environment with a Docker image with Conda dependencies, or with a Dockerfile.\n",
    "\n",
    "To create an environment using a base Docker image, you can define the Conda dependencies in a conda.yml file:\n",
    "\n",
    "```yml\n",
    "name: basic-env-cpu\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.7\n",
    "  - scikit-learn\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - matplotlib\n",
    "  ```\n",
    "\n",
    "  Then, to create the environment, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "    conda_file=\"./src/conda.yml\",\n",
    "    name=\"deployment-environment\",\n",
    "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
    ")\n",
    "ml_client.environments.create_or_update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the deployment\n",
    "When you have your model files, scoring script, and environment, you can create the deployment.\n",
    "\n",
    "To deploy a model to an endpoint, you can specify the compute configuration with two parameters:\n",
    "\n",
    "- instance_type: Virtual machine (VM) size to use. Review the list of supported sizes.\n",
    "- instance_count: Number of instances to use.\n",
    "\n",
    "To deploy the model, use the ManagedOnlineDeployment class and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment, CodeConfiguration\n",
    "\n",
    "model = Model(path=\"./model\",\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=\"endpoint-example\",\n",
    "    model=model,\n",
    "    environment=\"deployment-environment\",\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./src\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Explore the reference documentation to [create a managed online deployment with the Python SDK v2](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.entities.managedonlinedeployment).\n",
    "\n",
    "You can deploy multiple models to an endpoint. To route traffic to a specific deployment, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete the endpoint and all associated deployments, run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=\"endpoint-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test managed online endpoints\n",
    "After deploying a real-time service, you can consume it from client applications to predict labels for new data cases.\n",
    "\n",
    "## Use the Azure Machine Learning studio\n",
    "You can list all endpoints in the Azure Machine Learning studio, by navigating to the Endpoints page. In the Real-time endpoints tab, all endpoints are shown.\n",
    "\n",
    "You can select an endpoint to review its details and deployment logs.\n",
    "\n",
    "Additionally, you can use the studio to test the endpoint.\n",
    "\n",
    "![alt text](assets/test-studio.png)\n",
    "\n",
    "## Use the Azure Machine Learning Python SDK\n",
    "For testing, you can also use the Azure Machine Learning Python SDK to invoke an endpoint.\n",
    "\n",
    "Typically, you send data to deployed model in JSON format with the following structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"data\":[\n",
    "      [0.1,2.3,4.1,2.0], // 1st case\n",
    "      [0.2,1.8,3.9,2.1],  // 2nd case,\n",
    "      ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The response from the deployed model is a JSON collection with a prediction for each case that was submitted in the data. The following code sample invokes an endpoint and displays the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "response = ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"sample-data.json\",\n",
    ")\n",
    "\n",
    "if response[1]=='1':\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print (\"No\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
