{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make data available in Azure Machine Learning\n",
    "Data is a fundamental element in any machine learning workload. You need data to train a model and you create data when using a model to generate predictions.\n",
    "\n",
    "To work with data in Azure Machine Learning, you can access data by using Uniform Resource Identifiers (URIs). When you work with a data source or a specific file or folder repeatedly, you can create datastores and data assets within the Azure Machine Learning workspace. Datastores and data assets allow you to securely store the connection information to your data.\n",
    "\n",
    "In this module, you learn how to create and use URIs, datastores, and data assets in Azure Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand URIs\n",
    "You can store data on your local device, or somewhere in the cloud. Wherever you store your data, you want to access the data when training machine learning models. To find and access data in Azure Machine Learning, you can use Uniform Resource Identifiers (URIs).\n",
    "\n",
    "A URI references the location of your data. For Azure Machine Learning to connect to your data, you need to prefix the URI with the appropriate protocol. There are three common protocols when working with data in the context of Azure Machine Learning:\n",
    "\n",
    "![alt text](assets/data.png)\n",
    "\n",
    "- http(s): Use for data stores publicly or privately in an Azure Blob Storage or publicly available http(s) location.\n",
    "- abfs(s): Use for data stores in an Azure Data Lake Storage Gen 2.\n",
    "- azureml: Use for data stored in a datastore.\n",
    "\n",
    "For example, you may create an Azure Blob Storage in Azure. To store data, you create a container named training-data. Within the container, you create a folder datastore-path. Within the folder, you store the CSV file diabetes.csv.\n",
    "\n",
    "![alt text](assets/blob-storage.png)\n",
    "\n",
    "When you want to access the data from the Azure Machine Learning workspace, you can use the path to the folder or file directly. When you want to connect to the folder or file directly, you can use the http(s) protocol. If the container is set to private, you'll need to provide some kind of authentication to get access to the data, like a Shared Access Signature (SAS).\n",
    "\n",
    "When you create a datastore in Azure Machine Learning, you'll store the connection and authentication information in the workspace. Then, to access the data in the container, you can use the azureml protocol.\n",
    "\n",
    ">A datastore is a reference to an existing storage account on Azure. Therefore, when you refer to data stored in a datastore, you may be referring to data being stored in an Azure Blob Storage or Azure Data Lake Storage. When you refer to the datastore however, you won't need to authenticate as the connection information stored with the datastore will be used by Azure Machine Learning.\n",
    "\n",
    "It's considered a best practice to avoid any sensitive data in your code, like authentication information. Therefore, whenever possible, you should work with datastores and data assets in Azure Machine Learning. However, during experimentation in notebooks, you may want to connect directly to a storage location to avoid unnecessary overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a datastore\n",
    "In Azure Machine Learning, datastores are abstractions for cloud data sources. They encapsulate the information needed to connect to data sources, and securely store this connection information so that you donâ€™t have to code it in your scripts.\n",
    "\n",
    "The benefits of using datastores are:\n",
    "\n",
    "- Provides easy-to-use URIs to your data storage.\n",
    "- Facilitates data discovery within Azure Machine Learning.\n",
    "- Securely stores connection information, without exposing secrets and keys to data scientists.\n",
    "\n",
    "When you create a datastore with an existing storage account on Azure, you have the choice between two different authentication methods:\n",
    "\n",
    "- Credential-based: Use a service principal, shared access signature (SAS) token or account key to authenticate access to your storage account.\n",
    "- Identity-based: Use your Microsoft Entra identity or managed identity.\n",
    "\n",
    "## Understand types of datastores\n",
    "Azure Machine Learning supports the creation of datastores for multiple kinds of Azure data source, including:\n",
    "- Azure Blob Storage\n",
    "- Azure File Share\n",
    "- Azure Data Lake (Gen 2)\n",
    "\n",
    "## Use the built-in datastores\n",
    "Every workspace has four built-in datastores (two connecting to Azure Storage blob containers, and two connecting to Azure Storage file shares), which are used as system storages by Azure Machine Learning.\n",
    "\n",
    "In most machine learning projects, you need to work with data sources of your own. For example, you can integrate your machine learning solution with data from existing applications or data engineering pipelines.\n",
    "\n",
    "## Create a datastore\n",
    "Datastores are attached to workspaces and are used to store connection information to storage services. When you create a datastore, you provide a name that can be used to retrieve the connection information.\n",
    "\n",
    "Datastores allow you to easily connect to storage services without having to provide all necessary details every time you want to read or write data. It also creates a protective layer if you want users to use the data, but not connect to the underlying storage service directly.\n",
    "\n",
    "## Create a datastore for an Azure Blob Storage container\n",
    "You can create a datastore through the graphical user interface, the Azure command-line interface (CLI), or the Python software development kit (SDK).\n",
    "\n",
    "Depending on the storage service you want to connect to, there are different options for Azure Machine Learning to authenticate.\n",
    "\n",
    "For example, when you want to create a datastore to connect to an Azure Blob Storage container, you can use an account key:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = AzureBlobDatastore(\n",
    "    \t\t\tname = \"blob_example\",\n",
    "    \t\t\tdescription = \"Datastore pointing to a blob container\",\n",
    "    \t\t\taccount_name = \"mytestblobstore\",\n",
    "    \t\t\tcontainer_name = \"data-container\",\n",
    "    \t\t\tcredentials = AccountKeyCredentials(\n",
    "        \t\t\taccount_key=\"XXXxxxXXXxXXXXxxXXX\"\n",
    "    \t\t\t),\n",
    ")\n",
    "ml_client.create_or_update(blob_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can create a datastore to connect to an Azure Blob Storage container by using a SAS token to authenticate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_datastore = AzureBlobDatastore(\n",
    "name=\"blob_sas_example\",\n",
    "description=\"Datastore pointing to a blob container\",\n",
    "account_name=\"mytestblobstore\",\n",
    "container_name=\"data-container\",\n",
    "credentials=SasTokenCredentials(\n",
    "sas_token=\"?xx=XXXX-XX-XX&xx=xxxx&xxx=xxx&xx=xxxxxxxxxxx&xx=XXXX-XX-XXXXX:XX:XXX&xx=XXXX-XX-XXXXX:XX:XXX&xxx=xxxxx&xxx=XXxXXXxxxxxXXXXXXXxXxxxXXXXXxxXXXXXxXXXXxXXXxXXxXX\"\n",
    "),\n",
    ")\n",
    "ml_client.create_or_update(blob_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Learn more about [how to create datastores to connect to other types of cloud storage solutions](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a data asset\n",
    "As a data scientist, you want to focus on training machine learning models. Though you need access to data as input for a machine learning model, you don't want to worry about how to get access. To simplify getting access to the data you want to work with, you can use data assets.\n",
    "\n",
    "## Understand data assets\n",
    "In Azure Machine Learning, data assets are references to where the data is stored, how to get access, and any other relevant metadata. You can create data assets to get access to data in datastores, Azure storage services, public URLs, or data stored on your local device.\n",
    "\n",
    "The benefits of using data assets are:\n",
    "- You can share and reuse data with other members of the team such that they don't need to remember file locations.\n",
    "- You can seamlessly access data during model training (on any supported compute type) without worrying about connection strings or data paths.\n",
    "- You can version the metadata of the data asset.\n",
    "\n",
    "There are three main types of data assets you can use:\n",
    "\n",
    "- URI file: Points to a specific file.\n",
    "- URI folder: Points to a folder.\n",
    "- MLTable: Points to a folder or file, and includes a schema to read as tabular data.\n",
    "\n",
    ">URI stands for Uniform Resource Identifier and stands for a storage location on your local computer, Azure Blob or Data Lake Storage, publicly available https location, or even an attached datastore.\n",
    "\n",
    "## When to use data assets\n",
    "Data assets are most useful when executing machine learning tasks as Azure Machine Learning jobs. As a job, you can run a Python script that takes inputs and generates outputs. A data asset can be parsed as both an input or output of an Azure Machine Learning job.\n",
    "\n",
    "Letâ€™s take a look at each of the types of data assets, how to create them, and how to use the data asset in a job.\n",
    "\n",
    "## Create a URI file data asset\n",
    "\n",
    "A URI file data asset points to a specific file. Azure Machine Learning only stores the path to the file, which means you can point to any type of file. When you use the data asset, you specify how you want to read the data, which depends on the type of data you're connecting to.\n",
    "\n",
    "The supported paths you can use when creating a URI file data asset are:\n",
    "- Local: `./<path>`\n",
    "- Azure Blob Storage: `wasbs://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>`\n",
    "- Azure Data Lake Storage (Gen 2): `abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>`\n",
    "- Datastore: `azureml://datastores/<datastore_name>/paths/<folder>/<file>`\n",
    "\n",
    ">When you create a data asset and point to a file or folder stored on your local device, a copy of the file or folder will be uploaded to the default datastore workspaceblobstore. You can find the file or folder in the LocalUpload folder. By uploading a copy, you'll still be able to access the data from the Azure Machine Learning workspace, even when the local device on which the data is stored is unavailable.\n",
    "\n",
    "To create a URI file data asset, you can use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<supported-path>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"<description>\",\n",
    "    name=\"<name>\",\n",
    "    version=\"<version>\"\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you parse the URI file data asset as input in an Azure Machine Learning job, you first need to read the data before you can work with it.\n",
    "\n",
    "Imagine you create a Python script you want to run as a job, and you set the value of the input parameter input_data to be the URI file data asset (which points to a CSV file). You can read the data by including the following code in your Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "df = pd.read_csv(args.input_data)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your URI file data asset points to a different type of file, you need to use the appropriate Python code to read the data. For example, if instead of CSV files, you're working with JSON files, you'd use pd.read_json() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a URI folder data asset\n",
    "A URI folder data asset points to a specific folder. It works similar to a URI file data asset and supports the same paths.\n",
    "\n",
    "To create a URI folder data asset with the Python SDK, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<supported-path>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"<description>\",\n",
    "    name=\"<name>\",\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you parse the URI folder data asset as input in an Azure Machine Learning job, you first need to read the data before you can work with it.\n",
    "Imagine you create a Python script you want to run as a job, and you set the value of the input parameter input_data to be the URI folder data asset (which points to multiple CSV files). You can read all CSV files in the folder and concatenate them, which you can do by including the following code in your Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_path = args.input_data\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MLTable data asset\n",
    "A MLTable data asset allows you to point to tabular data. When you create a MLTable data asset, you specify the schema definition to read the data. As the schema is already defined and stored with the data asset, you don't have to specify how to read the data when you use it.\n",
    "\n",
    "Therefore, you want to use a MLTable data asset when the schema of your data is complex or changes frequently. Instead of changing how to read the data in every script that uses the data, you only have to change it in the data asset itself.\n",
    "\n",
    "When you define the schema when creating a MLTable data asset, you can also choose to only specify a subset of the data.\n",
    "\n",
    "For certain features in Azure Machine Learning, like Automated Machine Learning, you need to use a MLTable data asset, as Azure Machine Learning needs to know how to read the data.\n",
    "\n",
    "To define the schema, you can include a MLTable file in the same folder as the data you want to read. The MLTable file includes the path pointing to the data you want to read, and how to read the data:\n",
    "\n",
    "```yaml\n",
    "type: mltable\n",
    "\n",
    "paths:\n",
    "  - pattern: ./*.txt\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      encoding: ascii\n",
    "      header: all_files_same_headers\n",
    "```\n",
    "\n",
    ">Learn more on [how to create the MLTable file and which transformations you can include](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-mltable).\n",
    "\n",
    "To create a MLTable data asset with the Python SDK, you can use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<path-including-mltable-file>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.MLTABLE,\n",
    "    description=\"<description>\",\n",
    "    name=\"<name>\",\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you parse a MLTable data asset as input to a Python script you want to run as an Azure Machine Learning job, you can include the following code to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import mltable\n",
    "import pandas\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "tbl = mltable.load(args.input_data)\n",
    "df = tbl.to_pandas_dataframe()\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach is to convert the tabular data to a Pandas data frame. However, you can also convert the data to a Spark data frame if that suits your workload better."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
